{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob,os,re,math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "get_ipython().magic(u'matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    l=text.split()\n",
    "    punctuations=\",.?!\\\"\\':;-\"\n",
    "    l=[s.strip(punctuations).lower() for s in l]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk(text, length, overlap=False):\n",
    "    if overlap==False:\n",
    "        overlap=length\n",
    "    starts=range(0,len(text),overlap)\n",
    "    ans=[text[i:i+n] for i in starts]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_freq(path):\n",
    "    #Create a corpus\n",
    "    filenames=glob.glob(path)\n",
    "    corpus=[tokenize(open(f,'r').read()) for f in filenames]\n",
    "    #Build a dictionary of tokens\n",
    "    tf={}\n",
    "    for text in corpus:\n",
    "        for token in text:\n",
    "            if token in tf:\n",
    "                tf[token]+=1\n",
    "            else:\n",
    "                tf[token]=1\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_freq(path):\n",
    "    \n",
    "    ## build corpus\n",
    "    filenames=glob.glob(path)\n",
    "    corpus=[tokenize(open(f,'r').read()) for f in filenames]\n",
    "    \n",
    "    ## create dictionary with document frequency: unique word in a text\n",
    "    doc_freq={}\n",
    "    for text in corpus:\n",
    "        text=set(text) \n",
    "        for wd in text:\n",
    "            if wd in doc_freq.keys():\n",
    "                doc_freq[wd]+=1\n",
    "            else:\n",
    "                doc_freq[wd]=1\n",
    "                    \n",
    "    return doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(features, text):\n",
    "    ##text: A list of string\n",
    "    ## feature: list\n",
    "    f_counts = []\n",
    "    for f in features:\n",
    "        count=len([wd for wd in text if wd==f])\n",
    "        f_counts.append(count/float(len(text)))\n",
    "    return f_counts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(filepath,corpuspath):\n",
    "    #tf_idf score dict\n",
    "        tf_dict =tf(filepath)\n",
    "        df_dict = df(corpuspath)\n",
    "        n=0\n",
    "        for i in glob.glob(corpuspath):\n",
    "            n+=1\n",
    "        score={}\n",
    "        for token in tf_dict.keys():\n",
    "            score[token]=round(tf_dict[token]\n",
    "                                 *math.log(float(n)/df_dict[token]))\n",
    "        return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(c1,c2):\n",
    "    ans=0\n",
    "    for i, v in enumerate(c1):\n",
    "        ans+=(v-c2[i])**2\n",
    "        ans=ans**0.5\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def type_token(text):\n",
    "    ans=len(set(text))/(float)(len(text))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(path,top_words):\n",
    "    corpus = []\n",
    "    for f in glob.glob(path):\n",
    "        text = open(f, 'r').read()\n",
    "        text = tokenize(text)\n",
    "        corpus.append(text)\n",
    "    print corpus[:2][:10]\n",
    "    tokens = {}\n",
    "    for t in corpus:\n",
    "        for word in t:\n",
    "            if word in tokens: \n",
    "                tokens[word] +=1\n",
    "            else:\n",
    "                tokens[word]=1\n",
    "    tupple_tokens =tokens.items()\n",
    "    sort(tupple_tokens, lambda x: [1])[:top_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
